{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3bac2d-7d69-4bf0-9cab-f6d5e9cc5aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Contextual AI integration and data visualization\n",
    "%pip install contextual-client matplotlib tqdm requests pandas dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eebf77-e20d-43cd-b812-06e606b1b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict\n",
    "from IPython.display import display, JSON\n",
    "import pandas as pd\n",
    "from contextual import ContextualAI\n",
    "import ast\n",
    "from IPython.display import display, Markdown\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffbe877-3de1-4faa-af64-482dfbfc1aec",
   "metadata": {},
   "source": [
    "API Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f401c8d-dda1-43c8-bf74-b5587e3254c1",
   "metadata": {},
   "source": [
    "Create your API key from app.contextual.ai , store it as .env file and then configure the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02630c66-eab5-4d3b-bc2b-5e409caf8e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key from .env\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize with your API key\n",
    "API_KEY = os.getenv(\"CONTEXTUAL_API_KEY\")\n",
    "client = ContextualAI(\n",
    "    api_key=API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1303154-240d-4a28-9d45-e0cf38a5ae90",
   "metadata": {},
   "source": [
    "Create your document datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c485c-4db1-4849-a105-604ff8ade4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_name = 'Financial_Demo_RAG'\n",
    "\n",
    "# Check if datastore exists\n",
    "datastores = client.datastores.list()\n",
    "existing_datastore = next((ds for ds in datastores if ds.name == datastore_name), None)\n",
    "\n",
    "if existing_datastore:\n",
    "    datastore_id = existing_datastore.id\n",
    "    print(f\"Using existing datastore with ID: {datastore_id}\")\n",
    "else:\n",
    "    result = client.datastores.create(name=datastore_name)\n",
    "    datastore_id = result.id\n",
    "    print(f\"Created new datastore with ID: {datastore_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1faac6-fe96-4a23-bdd8-40a80ae0f30d",
   "metadata": {},
   "source": [
    "Document Ingestion and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e10c0-e8a9-47ea-9b3a-8fde49eaa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# File list with corresponding GitHub URLs\n",
    "files_to_upload = [\n",
    "    # NVIDIA quarterly revnue 24/25\n",
    "    (\"A_Rev_by_Mkt_Qtrly_Trend_Q425.pdf\", \"https://raw.githubusercontent.com/ContextualAI/examples/refs/heads/main/08-ai-workshop/data/A_Rev_by_Mkt_Qtrly_Trend_Q425.pdf\"),\n",
    "    # NVIDIA quarterly revenue 22/23\n",
    "    (\"B_Q423-Qtrly-Revenue-by-Market-slide.pdf\", \"https://raw.githubusercontent.com/ContextualAI/examples/refs/heads/main/08-ai-workshop/data/B_Q423-Qtrly-Revenue-by-Market-slide.pdf\"),\n",
    "    # Spurious correlations report - fun example of graphs and statistical analysis\n",
    "    (\"C_Neptune.pdf\", \"https://raw.githubusercontent.com/ContextualAI/examples/refs/heads/main/08-ai-workshop/data/C_Neptune.pdf\"),\n",
    "    # Another spurious correlations report - fun example of graphs and statistical analysis\n",
    "    (\"D_Unilever.pdf\", \"https://raw.githubusercontent.com/ContextualAI/examples/refs/heads/main/08-ai-workshop/data/D_Unilever.pdf\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f3e9d-f711-4929-8253-313f339e0fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and ingest all files\n",
    "document_ids = []\n",
    "for filename, url in files_to_upload:\n",
    "    file_path = f'data/{filename}'\n",
    "\n",
    "    # Download file if it doesn't exist\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Fetching {file_path}\")\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {filename}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Upload to datastore\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            ingestion_result = client.datastores.documents.ingest(datastore_id, file=f)\n",
    "            document_id = ingestion_result.id\n",
    "            document_ids.append(document_id)\n",
    "            print(f\"Successfully uploaded {filename} to datastore {datastore_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {filename}: {str(e)}\")\n",
    "\n",
    "print(f\"Successfully uploaded {len(document_ids)} files to datastore\")\n",
    "print(f\"Document IDs: {document_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fecaea8-cf76-4edf-9fda-94e094f0ff26",
   "metadata": {},
   "source": [
    "Agent Creation and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f64d7-75d6-4d5f-9592-157e52070f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ystem_prompt = '''\n",
    "You are a helpful AI assistant created by Contextual AI to answer questions about relevant documentation provided to you. Your responses should be precise, accurate, and sourced exclusively from the provided information. Please follow these guidelines:\n",
    "* Only use information from the provided documentation. Avoid opinions, speculation, or assumptions.\n",
    "* Use the exact terminology and descriptions found in the provided content.\n",
    "* Keep answers concise and relevant to the user's question.\n",
    "* Use acronyms and abbreviations exactly as they appear in the documentation or query.\n",
    "* Apply markdown if your response includes lists, tables, or code.\n",
    "* Directly answer the question, then STOP. Avoid additional explanations unless specifically relevant.\n",
    "* If the information is irrelevant, simply respond that you don't have relevant documentation and do not provide additional comments or suggestions. Ignore anything that cannot be used to directly answer this query.\n",
    "'''\n",
    "\n",
    "agent_name = \"Demo\"\n",
    "\n",
    "# Get list of existing agents\n",
    "agents = client.agents.list()\n",
    "\n",
    "# Check if agent already exists\n",
    "existing_agent = next((agent for agent in agents if agent.name == agent_name), None)\n",
    "\n",
    "if existing_agent:\n",
    "    agent_id = existing_agent.id\n",
    "    print(f\"Using existing agent with ID: {agent_id}\")\n",
    "else:\n",
    "    print(\"Creating new agent\")\n",
    "    app_response = client.agents.create(\n",
    "        name=agent_name,\n",
    "        description=\"Helpful Grounded AI Assistant\",\n",
    "        datastore_ids=[datastore_id],\n",
    "        agent_configs={\n",
    "        \"global_config\": {\n",
    "            \"enable_multi_turn\": False # Turning this off for deterministic responses for this demo\n",
    "        }\n",
    "        },\n",
    "        suggested_queries=[\n",
    "            \"What was NVIDIA's annual revenue by fiscal year 2022 to 2025?\",\n",
    "            \"When did NVIDIA's data center revenue overtake gaming revenue?\",\n",
    "            \"What's the correlation between the distance between Neptune and the Sun and Burglary rates in the US?\",\n",
    "            \"What's the correlation between Global revenue generated by Unilever Group and Google searches for 'lost my wallet'?\",\n",
    "            \"Does this imply that Unilever Group's revenue is derived from lost wallets?\",\n",
    "            \"What's the correlation between the distance between Neptune and the Sun and Global revenue generated by Unilever Group?\"\n",
    "        ]\n",
    "    )\n",
    "    agent_id = app_response.id\n",
    "    print(f\"Agent ID created: {agent_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c28791-daa8-4804-a241-70b70f9e45b3",
   "metadata": {},
   "source": [
    "Query the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b112a-2936-4f3b-af49-3032b9e2063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = client.agents.query.create(\n",
    "    agent_id=agent_id,\n",
    "    messages=[{\n",
    "        \"content\": \"What was NVIDIA's annual revenue by fiscal year 2022 to 2025?\",\n",
    "        \"role\": \"user\"\n",
    "    }]\n",
    ")\n",
    "print(query_result.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcaca6c-4032-405b-b0fe-c6666cf15b2b",
   "metadata": {},
   "source": [
    "Optional - Displaying the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fceb89-0db0-4c4f-99ab-872689d1e05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_base64_image(base64_string, title=\"Document\"):\n",
    "    # Decode base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "\n",
    "    # Create PIL Image object\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Display using matplotlib\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    return img\n",
    "\n",
    "# Retrieve and display all referenced documents\n",
    "for i, retrieval_content in enumerate(query_result.retrieval_contents):\n",
    "    print(f\"\\n--- Processing Document {i+1} ---\")\n",
    "\n",
    "    # Get retrieval info for this document\n",
    "    ret_result = client.agents.query.retrieval_info(\n",
    "        message_id=query_result.message_id,\n",
    "        agent_id=agent_id,\n",
    "        content_ids=[retrieval_content.content_id]\n",
    "    )\n",
    "\n",
    "    print(f\"Retrieval Info for Document {i+1}:\")\n",
    "\n",
    "    # Display the document image\n",
    "    if ret_result.content_metadatas and ret_result.content_metadatas[0].page_img:\n",
    "        base64_string = ret_result.content_metadatas[0].page_img\n",
    "        img = display_base64_image(base64_string, f\"Document {i+1}\")\n",
    "    else:\n",
    "        print(f\"No image available for Document {i+1}\")\n",
    "\n",
    "print(f\"\\nTotal documents processed: {len(query_result.retrieval_contents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b9991a-a6a6-42bc-9630-7629b193945d",
   "metadata": {},
   "source": [
    "OPTIONAL - COMPONENTS AVAILABLE IN CONTEXTUAL AI  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38c866-3fdf-4406-91b9-d084a55edcd6",
   "metadata": {},
   "source": [
    "Document Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5df40b-a6dd-4d55-969e-472e124de7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Attention is All You Need paper from arXiv\n",
    "url = \"https://arxiv.org/pdf/1706.03762\"\n",
    "file_path = \"data/attention-is-all-you-need.pdf\"\n",
    "\n",
    "with open(file_path, \"wb\") as f:\n",
    "    f.write(requests.get(url).content)\n",
    "\n",
    "print(f\"Downloaded paper to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5063a7d-bcb1-4c67-85d3-73a64aa20c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup headers for direct API calls\n",
    "base_url = \"https://api.contextual.ai/v1\"\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"authorization\": f\"Bearer {API_KEY}\"\n",
    "}\n",
    "\n",
    "# Submit parse job\n",
    "url = f\"{base_url}/parse\"\n",
    "\n",
    "config = {\n",
    "    \"parse_mode\": \"standard\",\n",
    "    \"figure_caption_mode\": \"concise\",\n",
    "    \"enable_document_hierarchy\": True,\n",
    "    \"page_range\": \"0-5\",\n",
    "}\n",
    "\n",
    "with open(file_path, \"rb\") as fp:\n",
    "    file = {\"raw_file\": fp}\n",
    "    result = requests.post(url, headers=headers, data=config, files=file)\n",
    "    response = json.loads(result.text)\n",
    "\n",
    "job_id = response['job_id']\n",
    "print(f\"Parse job submitted with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a572edc4-fcef-4c5b-b753-85c9cd29195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parse results\n",
    "url = f\"{base_url}/parse/jobs/{job_id}/results\"\n",
    "\n",
    "output_types = [\"markdown-per-page\"]\n",
    "\n",
    "result = requests.get(\n",
    "    url,\n",
    "    headers=headers,\n",
    "    params={\"output_types\": \",\".join(output_types)},\n",
    ")\n",
    "\n",
    "result = json.loads(result.text)\n",
    "print(f\"Parse job is {result['status']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c1ed1-bb33-4585-9bb3-1723a625ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first page's parsed markdown\n",
    "if 'pages' in result and len(result['pages']) > 0:\n",
    "    display(Markdown(result['pages'][0]['markdown']))\n",
    "else:\n",
    "    print(\"No parsed content available. Please check if the job completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d99e50e-ca94-41b7-be5e-51643af20f0e",
   "metadata": {},
   "source": [
    "Instruction-Following Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b8286d-3cac-4116-b193-e792ffbd58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our query and instruction\n",
    "query = \"What is the current enterprise pricing for the RTX 5090 GPU for bulk orders?\"\n",
    "\n",
    "instruction = \"Prioritize internal sales documents over market analysis reports. More recent documents should be weighted higher. Enterprise portal content supersedes distributor communications.\"\n",
    "\n",
    "# Sample documents with conflicting information\n",
    "documents = [\n",
    "    \"Following detailed cost analysis and market research, we have implemented the following changes: AI training clusters will see a 15% uplift in raw compute performance, enterprise support packages are being restructured, and bulk procurement programs (100+ units) for the RTX 5090 Enterprise series will operate on a $2,899 baseline.\",\n",
    "    \"Enterprise pricing for the RTX 5090 GPU bulk orders (100+ units) is currently set at $3,100-$3,300 per unit. This pricing for RTX 5090 enterprise bulk orders has been confirmed across all major distribution channels.\",\n",
    "    \"RTX 5090 Enterprise GPU requires 450W TDP and 20% cooling overhead.\"\n",
    "]\n",
    "\n",
    "# Metadata that helps distinguish document sources and dates\n",
    "metadata = [\n",
    "    \"Date: January 15, 2025. Source: NVIDIA Enterprise Sales Portal. Classification: Internal Use Only\",\n",
    "    \"TechAnalytics Research Group. 11/30/2023.\",\n",
    "    \"January 25, 2025; NVIDIA Enterprise Sales Portal; Internal Use Only\"\n",
    "]\n",
    "\n",
    "# Use the instruction-following reranker model\n",
    "model = \"ctxl-rerank-en-v1-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eaf786-d292-49e5-9a89-10565cf614e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the reranking\n",
    "rerank_response = client.rerank.create(\n",
    "    query=query,\n",
    "    instruction=instruction,\n",
    "    documents=documents,\n",
    "    metadata=metadata,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "print(\"Reranking Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(rerank_response.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e5f82-f788-4cb4-91a3-7b3d9ae87b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ranked results in a more readable format\n",
    "print(\"\\nRanked Documents (by relevance score):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, result in enumerate(rerank_response.results):\n",
    "    doc_index = result.index\n",
    "    score = result.relevance_score\n",
    "\n",
    "    print(f\"\\nRank {i+1}: Score {score:.4f}\")\n",
    "    print(f\"Document {doc_index + 1}:\")\n",
    "    print(f\"Content: {documents[doc_index][:100]}...\")\n",
    "    print(f\"Metadata: {metadata[doc_index]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e6fb46-0729-438e-b5ec-9927a38dd896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank without instructions for comparison\n",
    "rerank_no_instruction = client.rerank.create(\n",
    "    query=query,\n",
    "    documents=documents,\n",
    "    metadata=metadata,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "print(\"\\nRanking WITHOUT Instructions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, result in enumerate(rerank_no_instruction.results):\n",
    "    doc_index = result.index\n",
    "    score = result.relevance_score\n",
    "\n",
    "    print(f\"Rank {i+1}: Document {doc_index + 1}, Score: {score:.4f}\")\n",
    "\n",
    "print(\"\\nRanking WITH Instructions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, result in enumerate(rerank_response.results):\n",
    "    doc_index = result.index\n",
    "    score = result.relevance_score\n",
    "\n",
    "    print(f\"Rank {i+1}: Document {doc_index + 1}, Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93c9062-3795-4837-9607-c2e47929bfde",
   "metadata": {},
   "source": [
    "Grounded Language Model (GLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e261078-9e4b-46ca-bead-b152f7a6546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example conversation messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What are the most promising renewable energy technologies for addressing climate change in developing nations?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Based on current research, solar and wind power show significant potential for developing nations due to decreasing costs and scalability. Would you like to know more about specific implementation challenges and success stories?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Yes, please tell me about successful solar implementations in Africa and their economic impact, particularly focusing on rural electrification.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Detailed knowledge sources with varied information\n",
    "knowledge = [\n",
    "    \"\"\"According to the International Renewable Energy Agency (IRENA) 2023 report:\n",
    "    - Solar PV installations in Africa reached 10.4 GW in 2022\n",
    "    - The cost of solar PV modules decreased by 80% between 2010 and 2022\n",
    "    - Rural electrification projects have provided power to 17 million households\"\"\",\n",
    "\n",
    "    \"\"\"Case Study: Rural Electrification in Kenya (2020-2023)\n",
    "    - 2.5 million households connected through mini-grid systems\n",
    "    - Average household income increased by 35% after electrification\n",
    "    - Local businesses reported 47% growth in revenue\n",
    "    - Education outcomes improved with 3 additional study hours per day\"\"\",\n",
    "\n",
    "    \"\"\"Economic Analysis of Solar Projects in Sub-Saharan Africa:\n",
    "    - Job creation: 25 jobs per MW of installed capacity\n",
    "    - ROI average of 12-15% for mini-grid projects\n",
    "    - Reduced energy costs by 60% compared to diesel generators\n",
    "    - Carbon emissions reduction: 2.3 million tonnes CO2 equivalent\"\"\",\n",
    "\n",
    "    \"\"\"Technical Specifications and Best Practices:\n",
    "    - Optimal solar panel efficiency in African climate conditions: 15-22%\n",
    "    - Battery storage requirements: 4-8 kWh per household\n",
    "    - Maintenance costs: $0.02-0.04 per kWh\n",
    "    - Expected system lifetime: 20-25 years\"\"\",\n",
    "\n",
    "    \"\"\"Social Impact Assessment:\n",
    "    - Women-led businesses increased by 45% in electrified areas\n",
    "    - Healthcare facilities reported 72% improvement in service delivery\n",
    "    - Mobile money usage increased by 60%\n",
    "    - Agricultural productivity improved by 28% with electric irrigation\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf27070-e940-4c2b-9059-b56e13e9d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for direct API call\n",
    "base_url = \"https://api.contextual.ai/v1\"\n",
    "generate_api_endpoint = f\"{base_url}/generate\"\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"authorization\": f\"Bearer {API_KEY}\"\n",
    "}\n",
    "\n",
    "# Configure the GLM request\n",
    "payload = {\n",
    "    \"model\": \"v1\",\n",
    "    \"messages\": messages,\n",
    "    \"knowledge\": knowledge,\n",
    "    \"avoid_commentary\": False,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "\n",
    "# Generate the response\n",
    "generate_response = requests.post(generate_api_endpoint, json=payload, headers=headers)\n",
    "\n",
    "print(\"GLM Grounded Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(generate_response.json()['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c8dd9-8f8e-4b6f-843b-4f79a3589621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response with avoid_commentary enabled\n",
    "payload_no_commentary = payload.copy()\n",
    "payload_no_commentary[\"avoid_commentary\"] = True\n",
    "\n",
    "generate_response_no_commentary = requests.post(generate_api_endpoint, json=payload_no_commentary, headers=headers)\n",
    "\n",
    "print(\"GLM Response (with avoid_commentary=True):\")\n",
    "print(\"=\" * 50)\n",
    "print(generate_response_no_commentary.json()['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e94f2-fc95-4ac1-bfbd-ba8561d39dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COMPARISON:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. Standard GLM Response (avoid_commentary=False):\")\n",
    "print(\"-\" * 50)\n",
    "print(generate_response.json()['response'])\n",
    "\n",
    "print(\"\\n\\n2. Strict Grounding Mode (avoid_commentary=True):\")\n",
    "print(\"-\" * 50)\n",
    "print(generate_response_no_commentary.json()['response'])\n",
    "\n",
    "print(\"\\n\\nKey Differences:\")\n",
    "print(\"- Standard mode may include helpful context and commentary\")\n",
    "print(\"- Strict mode focuses purely on information from knowledge sources\")\n",
    "print(\"- Both modes maintain strong grounding in provided sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3bd0a-2a03-491a-a2b8-fd29be38e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query about a completely different topic\n",
    "different_query = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What are the latest developments in quantum computing hardware?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Same renewable energy knowledge (irrelevant to quantum computing)\n",
    "irrelevant_payload = {\n",
    "    \"model\": \"v1\",\n",
    "    \"messages\": different_query,\n",
    "    \"knowledge\": knowledge,  # Still about renewable energy\n",
    "    \"avoid_commentary\": False,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "\n",
    "irrelevant_response = requests.post(generate_api_endpoint, json=irrelevant_payload, headers=headers)\n",
    "\n",
    "print(\"GLM Response to Irrelevant Query:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Query: What are the latest developments in quantum computing hardware?\")\n",
    "print(\"Knowledge provided: Renewable energy information\")\n",
    "print(\"\\nGLM Response:\")\n",
    "print(irrelevant_response.json()['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a890d-a074-46f4-a349-8909f1bac2fa",
   "metadata": {},
   "source": [
    "LMUnit: Natural Language Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de5670b-b792-4031-aebe-1ee9ab837d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example\n",
    "query = \"What was NVIDIA's Data Center revenue in Q4 FY25?\"\n",
    "\n",
    "response = \"\"\"NVIDIA's Data Center revenue for Q4 FY25 was $35,580 million.\n",
    "\n",
    "This represents a significant increase from the previous quarter (Q3 FY25) when Data Center revenue was $30,771 million.\n",
    "\n",
    "The full quarterly trend for Data Center revenue in FY25 was:\n",
    "- Q4 FY25: $35,580 million\n",
    "- Q3 FY25: $30,771 million\n",
    "- Q2 FY25: $26,272 million\n",
    "- Q1 FY25: $22,563 million\"\"\"\n",
    "\n",
    "unit_test = \"Does the response avoid unnecessary information?\"\n",
    "\n",
    "# Evaluate with LMUnit\n",
    "result = client.lmunit.create(\n",
    "    query=query,\n",
    "    response=response,\n",
    "    unit_test=unit_test\n",
    ")\n",
    "\n",
    "print(f\"Unit Test: {unit_test}\")\n",
    "print(f\"Score: {result.score}/5\")\n",
    "print(f\"\\nAnalysis: The response includes additional quarterly trends beyond the specific Q4 request,\")\n",
    "print(f\"which explains the lower score for avoiding unnecessary information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c1cb8e-0a5b-4e93-acb5-3227ea0af9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define comprehensive unit tests for quantitative reasoning\n",
    "unit_tests = [\n",
    "    \"Does the response accurately extract specific numerical data from the documents?\",\n",
    "    \"Does the agent properly distinguish between correlation and causation?\",\n",
    "    \"Are multi-document comparisons performed correctly with accurate calculations?\",\n",
    "    \"Are potential limitations or uncertainties in the data clearly acknowledged?\",\n",
    "    \"Are quantitative claims properly supported with specific evidence from the source documents?\",\n",
    "    \"Does the response avoid unnecessary information?\"\n",
    "]\n",
    "\n",
    "# Create category mapping for visualization\n",
    "test_categories = {\n",
    "    'Does the response accurately extract specific numerical data': 'ACCURACY',\n",
    "    'Does the agent properly distinguish between correlation and causation': 'CAUSATION',\n",
    "    'Are multi-document comparisons performed correctly': 'SYNTHESIS',\n",
    "    'Are potential limitations or uncertainties in the data': 'LIMITATIONS',\n",
    "    'Are quantitative claims properly supported with specific evidence': 'EVIDENCE',\n",
    "    'Does the response avoid unnecessary information': 'RELEVANCE'\n",
    "}\n",
    "\n",
    "print(\"Unit Test Framework:\")\n",
    "print(\"=\" * 50)\n",
    "for i, test in enumerate(unit_tests, 1):\n",
    "    category = next((v for k, v in test_categories.items() if k.lower() in test.lower()), 'OTHER')\n",
    "    print(f\"{i}. {category}: {test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc5033-9f3e-4782-b465-8360f8d64e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample evaluation dataset\n",
    "evaluation_data = [\n",
    "    {\n",
    "        \"prompt\": \"What was NVIDIA's Data Center revenue in Q4 FY25?\",\n",
    "        \"response\": \"NVIDIA's Data Center revenue for Q4 FY25 was $35,580 million. This represents a significant increase from the previous quarter.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is the correlation coefficient between Neptune's distance from the Sun and US burglary rates?\",\n",
    "        \"response\": \"According to the Tyler Vigen spurious correlations dataset, there is a correlation coefficient of 0.87 between Neptune's distance from the Sun and US burglary rates. However, this is clearly a spurious correlation with no causal relationship.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How did NVIDIA's total revenue change from Q1 FY22 to Q4 FY25?\",\n",
    "        \"response\": \"NVIDIA's total revenue grew from $5.66 billion in Q1 FY22 to $60.9 billion in Q4 FY25, representing a massive increase driven primarily by AI and data center demand.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation_data)\n",
    "print(\"Sample Evaluation Dataset:\")\n",
    "print(eval_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564811b5-ae1c-4813-81d4-dccce93f6c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef run_unit_tests_with_progress(\n",
    "    df: pd.DataFrame,\n",
    "    unit_tests: List[str]\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run unit tests with progress tracking and error handling.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for idx in tqdm(range(len(df)), desc=\"Processing responses\"):\n",
    "        row = df.iloc[idx]\n",
    "        row_results = []\n",
    "\n",
    "        for test in unit_tests:\n",
    "            try:\n",
    "                result = client.lmunit.create(\n",
    "                    query=row['prompt'],\n",
    "                    response=row['response'],\n",
    "                    unit_test=test\n",
    "                )\n",
    "\n",
    "                row_results.append({\n",
    "                    'test': test,\n",
    "                    'score': result.score,\n",
    "                    'metadata': result.metadata if hasattr(result, 'metadata') else None\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with prompt {idx}, test '{test}': {e}\")\n",
    "                row_results.append({\n",
    "                    'test': test,\n",
    "                    'score': None,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "\n",
    "        results.append({\n",
    "            'prompt': row['prompt'],\n",
    "            'response': row['response'],\n",
    "            'test_results': row_results\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the evaluation\n",
    "print(\"Running comprehensive unit test evaluation...\")\n",
    "results = run_unit_tests_with_progress(eval_df, unit_tests)\n",
    "\n",
    "# Display detailed results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATION {i+1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Prompt: {result['prompt']}\")\n",
    "    print(f\"Response: {result['response'][:100]}...\")\n",
    "    print(\"\\nUnit Test Scores:\")\n",
    "\n",
    "    for test_result in result['test_results']:\n",
    "        if 'score' in test_result and test_result['score'] is not None:\n",
    "            category = next((v for k, v in test_categories.items() if k.lower() in test_result['test'].lower()), 'OTHER')\n",
    "            print(f\"  {category}: {test_result['score']:.2f}/5\")\n",
    "        else:\n",
    "            print(f\"  Error: {test_result.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30196648-45f8-479b-a91a-9a225e91b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_test_to_category(test_question: str) -> str:\n",
    "    \"\"\"Map the full test question to its category.\"\"\"\n",
    "    for key, value in test_categories.items():\n",
    "        if key.lower() in test_question.lower():\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def create_unit_test_plots(results: List[Dict], test_indices: Optional[List[int]] = None):\n",
    "    \"\"\"\n",
    "    Create polar plot(s) for unit test results.\n",
    "    \"\"\"\n",
    "    if test_indices is None:\n",
    "        test_indices = list(range(len(results)))\n",
    "    elif isinstance(test_indices, int):\n",
    "        test_indices = [test_indices]\n",
    "\n",
    "    categories = ['ACCURACY', 'CAUSATION', 'SYNTHESIS', 'LIMITATIONS', 'EVIDENCE', 'RELEVANCE']\n",
    "    angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False)\n",
    "    angles = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "    num_plots = len(test_indices)\n",
    "    fig = plt.figure(figsize=(6 * num_plots, 6))\n",
    "\n",
    "    for plot_idx, result_idx in enumerate(test_indices):\n",
    "        result = results[result_idx]\n",
    "        ax = plt.subplot(1, num_plots, plot_idx + 1, projection='polar')\n",
    "\n",
    "        scores = []\n",
    "        for category in categories:\n",
    "            score = None\n",
    "            for test_result in result['test_results']:\n",
    "                mapped_category = map_test_to_category(test_result['test'])\n",
    "                if mapped_category == category:\n",
    "                    score = test_result['score']\n",
    "                    break\n",
    "            scores.append(score if score is not None else 0)\n",
    "\n",
    "        scores = np.concatenate((scores, [scores[0]]))\n",
    "\n",
    "        ax.plot(angles, scores, 'o-', linewidth=2, color='blue')\n",
    "        ax.fill(angles, scores, alpha=0.25, color='blue')\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(categories)\n",
    "        ax.set_ylim(0, 5)\n",
    "        ax.grid(True)\n",
    "\n",
    "        for angle, score, category in zip(angles[:-1], scores[:-1], categories):\n",
    "            ax.text(angle, score + 0.2, f'{score:.1f}', ha='center', va='bottom')\n",
    "\n",
    "        prompt = result['prompt'][:50] + \"...\" if len(result['prompt']) > 50 else result['prompt']\n",
    "        ax.set_title(f\"Evaluation {result_idx + 1}\\n{prompt}\", pad=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Create visualizations\n",
    "if len(results) > 0:\n",
    "    fig = create_unit_test_plots(results)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86f203-5f10-40f7-9048-7f5794c00341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregate analysis\n",
    "all_scores = []\n",
    "for result in results:\n",
    "    for test_result in result['test_results']:\n",
    "        if 'score' in test_result and test_result['score'] is not None:\n",
    "            category = map_test_to_category(test_result['test'])\n",
    "            all_scores.append({\n",
    "                'category': category,\n",
    "                'score': test_result['score'],\n",
    "                'test': test_result['test']\n",
    "            })\n",
    "\n",
    "scores_df = pd.DataFrame(all_scores)\n",
    "\n",
    "if not scores_df.empty:\n",
    "    # Calculate average scores by category\n",
    "    avg_scores = scores_df.groupby('category')['score'].agg(['mean', 'std', 'count']).round(2)\n",
    "\n",
    "    print(\"\\nAggregate Performance by Category:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(avg_scores)\n",
    "\n",
    "    # Overall statistics\n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"Mean Score: {scores_df['score'].mean():.2f}/5\")\n",
    "    print(f\"Standard Deviation: {scores_df['score'].std():.2f}\")\n",
    "    print(f\"Total Evaluations: {len(scores_df)}\")\n",
    "else:\n",
    "    print(\"No valid scores to analyze\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
